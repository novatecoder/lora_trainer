# 📍 Project Roadmap: Lora-trainer

**Lora-trainer** 프로젝트의 목표는 `Qwen2.5-7B-Instruct` 모델을 기반으로, 사용자의 감정에 공감하고 혈액형별 개성을 가진 **'서포터 AI'**용 LoRA 어댑터를 구축하는 것입니다.

## 🏁 Phase 1: 기반 구축 및 환경 설정 (Environment & Setup)

* [ ] **프로젝트 초기화**: `pyproject.toml` 구성을 통한 `Unsloth`, `PyTorch`, `Transformers` 의존성 관리.
* [ ] **학습 환경 검증**: 로컬 GPU 환경에서 `Unsloth`를 이용한 베이스 모델(`Qwen2.5-7B`) 로드 및 하드웨어 가속 확인.
* [ ] **State 및 프롬프트 정의**: `expression_node`에서 사용할 최종 시스템 프롬프트 템플릿 및 입력 상태 구조 확정.

## 📊 Phase 2: 데이터셋 엔지니어링 (Data Engineering)

* [ ] **페르소나 가이드라인 수립**: A, B, O, AB형 각각의 언어적 습관(어미, 리액션 강도, 주요 단어) 상세 정의.
* [ ] **티키타카 데이터 생성**: GPT-4 등을 활용하여 짧은 문장 위주의 대화 데이터셋 생성 (형별 500~1,000세트 목표).
* [ ] **데이터 정제**: JSON 응답 형식(`text`, `emotion`) 준수 여부 및 한국어 자연스러움 필터링.
* [ ] **감정 태그 매핑**: 각 대화 샘플에 AI의 내부 감정 상태 데이터를 결합하여 학습 데이터셋 완성.

## 🚀 Phase 3: LoRA 어댑터 학습 (Model Training)

* [ ] **혈액형별 개별 학습**: 동일한 하이퍼파라미터와 시스템 프롬프트 구조 하에 4종의 LoRA 어댑터 제작.
* [ ] **중국어/환각 방지 튜닝**: 학습 과정에서 한국어 전용 데이터셋 비중을 높여 모델의 언어 일관성 확보.
* [ ] **Loss 모니터링**: `Tensorboard`를 활용한 학습 곡선 추적 및 과적합 방지.

## 🔗 Phase 4: 시스템 통합 및 테스트 (Integration & Testing)

* [ ] **어댑터 동적 로딩**: `supporter_ai` 프로젝트의 `expression_node`에서 혈액형별 어댑터를 실시간으로 교체(Switching)하는 기능 검증.
* [ ] **티키타카 성능 평가**: 모델이 혼자 길게 말하지 않고 사용자에게 질문을 던지거나 적절한 리액션을 하는지 정성적 평가.
* [ ] **JSON 파싱 안정성 테스트**: 다양한 입력 상황에서도 응답 형식이 깨지지 않는지 확인.

## 🌟 Phase 5: 최적화 및 고도화 (Advanced Optimization)

* [ ] **프롬프트 다이어트 최종 적용**: 추론 속도 향상을 위해 시스템 프롬프트에서 불필요한 토큰 제거.
* [ ] **멀티 턴 컨텍스트 강화**: 요약된 장기 기억(`Summary`)을 대화 속에 더 자연스럽게 녹여내는 기법 연구.
* [ ] **감정 수치 반영**: `Intensity` 값에 따라 "!"의 개수나 이모지 사용 빈도가 변하는 미세 조정 학습.

---

### 📅 향후 일정

* **Next Step**: 페르소나별 상세 대화 가이드라인 확정 및 데이터 생성 스크립트 작성.
* **Target Date**: Phase 3(학습 완료)까지 2주 내 완료 목표.